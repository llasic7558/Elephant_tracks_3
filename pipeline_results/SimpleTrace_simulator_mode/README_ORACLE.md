# Oracle Files - Quick Reference

This directory contains multiple oracle formats for different use cases.

## Files Overview

### 1. **oracle_event_stream.txt** ⭐ RECOMMENDED
**Clean, chronological event stream in human-readable format**

```
t3: alloc(id=458209687, size=24, site=62, thread=1950409828)
t8: free(id=38997010, size=40)
t12: alloc(id=38997010, size=40, site=80, thread=1950409828)
```

**Best for**: Memory simulators, GC research, understanding program behavior

### 2. **oracle.csv** ⭐ RECOMMENDED
**Same data as event stream, but in CSV format**

```csv
event_idx,event_type,obj_id,size,site,thread
3,alloc,458209687,24,62,1950409828
8,free,38997010,40,,
12,alloc,38997010,40,80,1950409828
```

**Best for**: Excel analysis, pandas/R scripts, statistical tools

### 3. **trace_with_deaths_oracle**
**ET3 trace format with death records inserted**

```
D 458209687 1950409828 2 24
M 35 0 1950409828
N 458209687 24 3 62 0 1950409828
```

**Best for**: Tools expecting ET3 trace format, legacy compatibility

### 4. **deaths_with_size.txt**
**Raw death records from simulator**

```
D 458209687 at time 2 (size: 24 bytes, type: TODO)
D 38997010 at time 8 (size: 40 bytes, type: TODO)
```

**Best for**: Debugging, understanding simulator output

## Quick Start

### Python Analysis
```python
import pandas as pd

# Load oracle
df = pd.read_csv('oracle.csv')

# Analyze allocations
allocs = df[df['event_type'] == 'alloc']
print(f"Total allocated: {allocs['size'].sum()} bytes")
print(f"Avg object size: {allocs['size'].mean():.1f} bytes")

# Track live objects over time
live = {}
for _, row in df.iterrows():
    if row['event_type'] == 'alloc':
        live[row['obj_id']] = row['size']
    else:  # free
        live.pop(row['obj_id'], None)
    print(f"t{row['event_idx']}: {sum(live.values())} bytes live")
```

### Shell Analysis
```bash
# Count events
grep 'alloc' oracle_event_stream.txt | wc -l  # 16 allocations
grep 'free' oracle_event_stream.txt | wc -l   # 31 frees

# Find largest allocation
grep 'alloc' oracle_event_stream.txt | \
  sed 's/.*size=\([0-9]*\).*/\1/' | sort -n | tail -1

# Average object size
awk -F',' '$2=="alloc" {sum+=$4; count++} END {print sum/count}' oracle.csv
```

### Memory Simulator (Python)
```python
def simulate_heap(oracle_file):
    heap = {}
    peak = 0
    
    with open(oracle_file) as f:
        for line in f:
            if 'alloc' in line:
                obj_id = extract(line, 'id')
                size = extract(line, 'size')
                heap[obj_id] = size
            elif 'free' in line:
                obj_id = extract(line, 'id')
                heap.pop(obj_id, None)
            
            current = sum(heap.values())
            peak = max(peak, current)
            print(f"Heap: {current} bytes")
    
    return peak
```

## Statistics

| Metric | Value |
|--------|-------|
| **Total events** | 47 |
| **Allocations** | 16 (program objects) |
| **Frees** | 31 (16 program + 15 phantom) |
| **Memory allocated** | 440 bytes |
| **Memory freed** | 500 bytes |
| **Phantom objects** | 15 (JVM infrastructure) |

## Generation

These files were generated by:
```bash
./create_oracle_trace.sh pipeline_results/SimpleTrace_simulator_mode
```

Which runs:
1. ET2 simulator to compute death times
2. `build_true_oracle.py` to create event stream
3. `oracle_to_csv.py` to generate CSV

## See Also

- `ORACLE_FORMAT.md` - Detailed format specification
- `ORACLE_EXPLANATION.md` - How oracle traces work
- `RESULTS_SUMMARY.md` - Simulator integration results

## Recommendation

**For most analyses, use `oracle.csv` or `oracle_event_stream.txt`.**

They provide the cleanest, most useful format with proper chronological ordering and rich metadata.
